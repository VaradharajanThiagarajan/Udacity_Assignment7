{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#                          weRateDog - wrangling project report\n",
    "\n",
    "This report explain the steps followed to wrangle the weRateDog datasets. \n",
    "To begin with let me list out the steps followed \n",
    "\n",
    "#### Steps followed\n",
    "\n",
    "1. Gather data\n",
    "2. Assess data\n",
    "3. Clean data\n",
    "\n",
    "###### Iterated  steps 2 and steps 3 until I felt dataset is tidy and also with less number of quality issues.I could not handle all the quality related issues, but I corrected the quality issues which I could.\n",
    "\n",
    "\n",
    "# Gather Data : \n",
    "\n",
    "###### In this step, I had to collect data from three sources. \n",
    "\n",
    "1. First source was the file  twitter_archive_enhanced.csv, which I manually downloaded and then stored the data from the file to a Twitter_file1 dataframe using the python pandas read function. \n",
    "\n",
    "2. Second source was the image predictions file which I programmatically downloaded from the given server link and then stored into Twitter_file2 dataframe.\n",
    "\n",
    "3. Third source was the challenging one, where I had to set up the Twitter API Account and then use the  Twitter API call based on the Tweet id's available in file1.API call had returned the data in specific format, which again parsed using python library and finally stored into the dataset called tweet_json.txt. Tricky part was that I was facing time out error which I had to handle using the wait_on_rate_limit = True,wait_on_rate_limit_notify = True parameters. Also for few tweetid's in file1, API call errored out. \n",
    "\n",
    "\n",
    "\n",
    "# Assess Data \n",
    "\n",
    "###### In order to Assess the gathered data , I followed two methods listed below\n",
    "\n",
    "1. Visual Assessment\n",
    "2. Programmatic Assessment\n",
    "\n",
    "\n",
    "###### Based on the visual assessment, I identified below quality as well as Tidyness issues. \n",
    "1. Invalid value in numerator (Very high value like 1770) and denominator (Has value zero) - quality issue\n",
    "2. Column Name has values like 'a','None' -which are invalid values - quality issue\n",
    "3. There are some retweeted records which need to be removed from the dataset - quality issue\n",
    "4. For a given Tweet_id , we can see that columns doggo as well as pupper are having values - Example consider the tweet about the dog name called  Maggie, it has got value doggo as well as pupper, for the same dog multiple dog stage value has been assigned - this is quality issue\n",
    "5. columns doggo,pupper,floofer,puppo are untidy columns - where we can replace them with single column called \"Dog_Type\"   - Tidy issue.  \n",
    "6. All the columns in image - Twitter_File2 dataframe are not necessary - Twitter_id and jpg_url are enough - Tidyness issue  \n",
    "7. There are only 2075 rows, which clearly explains missing entries (Ideally there should have been 2356 entries)- quality issue  \n",
    "8. To predict the dog_breed, we see there are three predictions, also if prediction is true/false and the confidence level as well. This seems untidy data and can be replaced with two columns - Dog_breed and confidence level. From the three given predictions, we can take the very first true prediction.  \n",
    "9. For few tweet_id, we could not fetch the favourite and retweets count - quality issue\n",
    "\n",
    "\n",
    "##### Based on the programmatic assessment, Identified the below quality issues as well as Tidyness issues. \n",
    "\n",
    "1. in_reply_to_status_id ,in_reply_to_user_id ,retweeted_status_id ,retweeted_status_user_id seem to have float datatype - quality issue.  \n",
    "2. retweeted_status_timestamp, timestamp should be datetime instead of object quality issue.  \n",
    "3. There are 181 records which has values for the column retweeted_status_id, from which we can infer that these records will be the retweeted records and not the original tweets. For our project , we can remove these records - Quality issue  \n",
    "4. There are duplicate entries based on image URL - quality issue  \n",
    "\n",
    "\n",
    "\n",
    "# Clean Data\n",
    "#### During the process of clean data, I had followed three basic rules as given below\n",
    "Rule 1: Define the clean process.  \n",
    "Rule 2: Code the clean process.  \n",
    "Rule 3: Validate the clean process.  \n",
    "\n",
    "As part of this process,  \n",
    "1.From file1, removed the retweeted records and then dropped the columns associated with those.  \n",
    "2.Transformed the untidy data to Tidy data by consolidating the Dog stage variable.  \n",
    "3.Changed the type of few columns and renamed them.  \n",
    "4.From the image predictions file, again transformed the data by consolidating the Dog_Breed variable.  \n",
    "5.From the file 3, removed the records corresponding to Tweetid which could not be accessed in the API call.  \n",
    "6.Merged all the three files into single merge file using the 'Tweet_id' as a key.  \n",
    "7.Then corrected the Dog name as well as Dog numerator and denominator Ratings in the merge file and thus created master archive file.  \n",
    "\n",
    "##### Point to be noted is that , I did not clean all the data quality issues. \n",
    "\n",
    "##### Finally based on the master archive file , I created three insights along with the visualisation\n",
    "\n",
    "#### Conclusion :\n",
    "#### To conclude, data wrangling is essential process which should be performed  before the exploratry Data analysis. As part of data wrangling  process, we identified few data quality issues and fixed few of those identified data quality issues. We also transformed the untidy data to tidy data. By this data wrangling process, we have set a very good foundation for the next indepth analysis of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
